---
title: "Predicting Errors in Barbell Lifts"
author: "Andrew Wagner"
date: "September 27, 2015"
output: html_document
---

```{r results='hide',warning=FALSE,echo=FALSE,message=FALSE,cache=TRUE}
load("rfmodel.RData")
load("rfmodel1.RData")
setwd("~/Coursera/MachineLearning")
allData <- read.csv("pml-training.csv")
require(dplyr)
require(ggplot2)
require(caret)
require(doMC)

subData <- tbl_df(allData) %>%
        dplyr::select(-contains("timestamp"),
               -contains("kurtosis"),
               -contains("skewness"),
               -contains("max"),
               -contains("min"),
               -contains("var"),
               -contains("stddev"),
               -contains("amplitude"),
               -contains("avg")) %>%
        dplyr::select(-X,-user_name,-new_window,-num_window)

asNumeric <- function(x) as.numeric(as.character(x))
factorsNumeric <- function(d){
        modifyList(d, lapply(d[, sapply(d, is.factor)],asNumeric))
}

subData <- cbind(subData$classe,factorsNumeric(subData))
subData <- subData[,-ncol(subData)]
colnames(subData)[1] <- "classe"
set.seed(321)
trainfix <- createDataPartition(y=subData$classe,p=.75,list=FALSE)
trainer = subData[trainfix,]
tester = subData[-trainfix,]
pred <- predict(rfmod,tester)
predtab <- table(pred,tester$classe)
miss <- 0
for(i in 1:5){
        miss <- miss + predtab[i,i]
}
oose <- miss/sum(predtab)
```

# Summary

The purpose of this document is to outline the steps taken in generating a prediction algorithm that can accurately assess and detect exercise errors given data gathered from wearable measurement devices. While a large amount of the statistical weight is lifted by R (thanks to some very well-designed packages), exploratory analysis into the expansive data set is essential to ensure that the final model relies solely on the relevant data and provides intelligible results.

# Process

Initial analysis of the output must first be predicated on the method of prediction that will ultimately be used, and must be tailored to idiosyncratic data. In this case, data was measured at specific time intervals and recorded along with timestamp data, activity code, and the user that performed the activities. Additionally, there are a series of measurements that represent aggregations of data over some span of time (though not *necessarily* specific to a single activity) and, though they could be valuable, were neither recorded for every observation nor were reliably filled with quantitative data. As an example, these data were measurements such as averages, standard deviations, local maximums, etc.

Since the test set data did not consistently have these data aggregations as observations, these variables were not included in the model. The timestamp data was also excepted from the model mainly because different activities were simply performed at different times, and there is no reason to believe that the time or date was controlled for across exercises when these data were gathered. Moreover, despite the fact that sequential actions may have some relationship to one another within an exercise, the atomic nature of the test set observations dictate that temporal analysis would not be relevant.

While username data is included in the test set, it was also excluded from this model. One could assume that these different users have different quirks that give rise to specific indicators in the data, it seems more reasonable to assume that this prediction algorithm should be intended to be accurate irrespective of the user. (The model was re-run including these data and performance was only marginally improved.)

R's read.csv function interpreted most columns as factor variables, so after removing all of the unwanted columns in the dataset they were converted to numeric values. This had the desired side-effect of converted any non-numeric values to NA.

Given that the model is predicting an outcome as a classification and not a quantitative variable, the random forest method was selected for analysis. Principal component signal extraction was used for preprocessing, centering and scaling the data. The random forest model resampled 25 times (or bootstraps). The data were partitioned at a rate of 75% for training and cross validation. While processing time was lengthy, this model proved effective on the first run through with an accuracy of `r round(rfmod$results[1,2]*100,2)`% and Kappa value of `r round(rfmod$results[1,3],2)`, which, when understanding the complexity and number of factors involved, seems to be satisfactory as an output. The out of sample error was `r round(oose*100,2)`%, when tested against the data set aside for validation




#Code

### Data Cleaning

```{r message=FALSE, warning=FALSE, eval=FALSE}
setwd("~/Coursera/MachineLearning")
allData <- read.csv("pml-training.csv")
require(dplyr)
require(ggplot2)
require(caret)
require(doMC)

subData <- tbl_df(allData) %>%
        dplyr::select(-contains("timestamp"),
               -contains("kurtosis"),
               -contains("skewness"),
               -contains("max"),
               -contains("min"),
               -contains("var"),
               -contains("stddev"),
               -contains("amplitude"),
               -contains("avg")) %>%
        dplyr::select(-X,-user_name,-new_window,-num_window)

asNumeric <- function(x) as.numeric(as.character(x))
factorsNumeric <- function(d){
        modifyList(d, lapply(d[, sapply(d, is.factor)],asNumeric))
}

subData <- cbind(subData$classe,factorsNumeric(subData))
subData <- subData[,-ncol(subData)]
colnames(subData)[1] <- "classe"

registerDoMC(cores = 8)

set.seed(321)
trainfix <- createDataPartition(y=subData$classe,p=.75,list=FALSE)
trainer = subData[trainfix,]
tester = subData[-trainfix,]

rfmod <- train(classe~., preProcess="pca",
              method="rf", data=trainer, seeds = c(23,45,67,89,12,34,56,78))

pred <- predict(rfmod,tester)
predtab <- table(pred,tester$classe)

rfmod

miss <- 0
for(i in 1:5){
        miss <- miss + predtab[i,i]
}
oose <- miss/sum(predtab)
```

# Figures

### Cross Validation table
```{r echo=FALSE}
predtab
```
This figure shows predicted values of the cross validation data vertically and the actual values horizontally. Fewer observations were misclassified as A than any other group, suggesting that this model generates more false positives than negatives.


### Model Details
```{r echo=FALSE}
rfmod

rfmod$preProcess
```
This is the R output outlining the details of the model, as well as pre-processing information from the model object.

### Confusion Matrix
```{r echo=FALSE}
rfmod$finalModel$confusion
```
This is a confusion matrix for the final model.
